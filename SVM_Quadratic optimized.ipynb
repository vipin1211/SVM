{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's biuld SVM for binary classification and use quadratic optimization package CVXOPT in python.\n",
    "\n",
    "- SVM supports regression too. Look into literature for regression usage\n",
    "- SVM supports multiclass classification too\n",
    "- We can combine muliple kernel to use data from different sources(video, image ,audio) and using right kernel to measure similarity and then combine.  See MKL https://en.wikipedia.org/wiki/Multiple_kernel_learning \n",
    "  \n",
    "- **One class SVM** are used for novelty (anamolies, outlier, noise etc) detection in fraud detection, medical abnormality, production abnormality etc. One can also use **ensemble.IsolationForest** from ensemble methods.\n",
    "- See this paper for a review of novelty detection methods. http://www.robots.ox.ac.uk/~davidc/pubs/NDreview2014.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's review some theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will implement  SVM. We know that SMV primal objective is\n",
    "\n",
    "$\\min_{w, w_0} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^{N} \\xi_i$ st $y_i(w^T +w_0) \\gt 1 $  for $\\forall i$\n",
    "\n",
    "Note the label is $(+1, -1)$ instead of $(0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Infact Dual problem(using Lagrangian) is written as \n",
    "$\\min_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\alpha_i \\alpha_j y_i y_j(x_i^T x_j) - 1^T \\alpha$ s.t $ \\alpha_i \\ge 0$ and $y^T\\alpha = 0$\n",
    "\n",
    "or \n",
    "\n",
    "$\\min_{\\alpha} \\frac{1}{2}\\alpha^T K \\alpha - 1^T \\alpha$ s.t $ \\alpha_i \\ge 0$ and $y^T\\alpha = 0$\n",
    "\n",
    "where  $\\frac{1}{2}\\alpha^T K \\alpha$ = $\\sum_{i=1}^N \\alpha_i \\alpha_j y_i y_j(x_i^T x_j)$ and $\\alpha$ is vector of $\\alpha_i$. Similar interpretation for $y$ and $y_i$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimization theory derivation here http://cs229.stanford.edu/notes/cs229-notes3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "dual problem depends only on innner product $x_i^Tx_j$ between $x_i, x_j$ . Hence we can replace it with a mercer kernal $\\mathcal{k}(x_i, x_j) = \\phi(x_i)^T \\phi(x_j)$ where $\\phi$ is the function\n",
    "guranteed by mercer theorem. You can think of it as a feature mapping function $x_i \\rightarrow \\phi(x_i)$.\n",
    "For kernel svm replace matrix K  using RBF kernel i.e $K_{ij} = y_i  y_j \\mathcal{k}(x_i,x_j)$ where  $\\mathcal{k}$ is any kernel. **Use RBF kernel for this assignment.**\n",
    "\n",
    "Complete dual problem in dual form is\n",
    "$\\min_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\alpha_i \\alpha_j y_i y_j \\mathcal{k}(x_i, x_j) - 1^T \\alpha$ s.t $ \\alpha_i \\ge 0$ and $y^T\\alpha = 0$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cvxopt Quadratic program solver (QP).\n",
    "\n",
    "cvxopt.solvers.qp(P, q[, G, h[, A, b[, solver[, initvals]]]])\n",
    "\n",
    "which solves the problem\n",
    "\n",
    "\n",
    "$\\min_{x} \\frac{1}{2}x^T P x {\\bf+} q^T x$ s.t $Gx \\preceq h$ and $Ax = b$. Note $\\preceq$ denotes componentwise inequality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we need to map to this interface. Clearly $x$ is $\\alpha$ and $K$ is $P$, $q$ is vector of -1.\n",
    "\n",
    "Now we have to build  inequality\n",
    "\n",
    "**(i)** $\\alpha_i \\ge 0$ or $ -\\alpha_i < 0 $ for all $i$ or in matrix \n",
    "form $- I\\alpha \\preceq {\\bf 0}$ where $I$ is  identity matrix of size $N \\times N$ and  ${\\bf 0}$ is vector of zeros of size N(number of samples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "Hence $G $ will be $\\begin{bmatrix} -I  \\end{bmatrix}$  and $h$ is $\\begin{bmatrix} {\\bf 0}   \\end{bmatrix} $\n",
    "\n",
    "\n",
    "$A$  is $y^T$ and $b$ is $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we solve for $\\alpha$, w is equal to $\\sum_{i} \\alpha_i y_i \\phi(x_i)$ and $w_0$  is equal to $\\frac{max_{j:y_j =-1} \\sum_{i} \\alpha_i y_i k(x_i, x_j) +  min_{j:y_j =1} \\sum_{i} \\alpha_i y_i k(x_i, x_j)}{2}$ \n",
    "\n",
    "We can use the sign$(w^T x_{test} + w_0)$  or  sign$(\\sum_{i}^{N} \\alpha_i  y_i \\mathcal{k}(x_i,x_{test}) + w_0)$ to predict label of test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's install  cvxopt package for optimization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helped me learn more about [Python Software for Convex Optimization](http://cvxopt.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['malignant', 'benign']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y= data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's convert dataset label to {-1, +1}\n",
    "We need to convert 0 to -1 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1= np.where(y==0, -1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 212)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y1 ==1), sum(y1 ==-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are ignoring data imbalance issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.811e+01, 3.928e+01, 1.885e+02, 2.501e+03, 1.634e-01, 3.454e-01,\n",
       "       4.268e-01, 2.012e-01, 3.040e-01, 9.744e-02, 2.873e+00, 4.885e+00,\n",
       "       2.198e+01, 5.422e+02, 3.113e-02, 1.354e-01, 3.960e-01, 5.279e-02,\n",
       "       7.895e-02, 2.984e-02, 3.604e+01, 4.954e+01, 2.512e+02, 4.254e+03,\n",
       "       2.226e-01, 1.058e+00, 1.252e+00, 2.910e-01, 6.638e-01, 2.075e-01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's standardize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X1 = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.97128765,  4.65188898,  3.97612984,  5.25052883,  4.77091122,\n",
       "        4.56842498,  4.24358882,  3.92792966,  4.48475086,  4.91091929,\n",
       "        8.90690934,  6.65527935,  9.46198577, 11.04184226,  8.02999927,\n",
       "        6.14348219, 12.0726804 ,  6.64960079,  7.07191706,  9.85159257,\n",
       "        4.09418939,  3.88590505,  4.28733746,  5.9301724 ,  3.95537411,\n",
       "        5.11287727,  4.7006688 ,  2.68587702,  6.04604135,  6.84685604])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.4, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((341,), (228,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_val_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114,), (114,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will Implement Kernel SVM using solver from cvxopt. Use validation set to select best model and  evaluate the model on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  rbf_kernel (x1,x2, sigma):\n",
    "    return np.exp(- np.linalg.norm(x1- x2)**2/(2.0* sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(341, 30) (341, 341)\n"
     ]
    }
   ],
   "source": [
    "# Let declare matrix K\n",
    "# We need to build it different sigma\n",
    "K = np.zeros((X_train.shape[0], X_train.shape[0]))\n",
    "print(X_train.shape, K.shape)\n",
    "n_samples = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q= cvxopt.matrix(-1*np.ones(n_samples))\n",
    "q.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build matrix  G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 341)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -I \n",
    "G = cvxopt.matrix(np.negative(np.identity(X_train.shape[0])))\n",
    "G.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build matrix h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(341, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = cvxopt.matrix(np.zeros((X_train.shape[0],1)), tc=\"d\")\n",
    "h.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note y_train contains integer label value (1, -1) but in cvopt library A needs to be double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 341)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = cvxopt.matrix(np.expand_dims(y_train, axis=0), tc = \"d\")\n",
    "A.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= cvxopt.matrix(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(x, svec_alphas, svecs, svec_labels, wo, sigma):\n",
    "    val = wo\n",
    "    for alpha, svec, lbl in  zip(svec_alphas, svecs, svec_labels):\n",
    "        val +=  alpha*lbl*rbf_kernel(x,svec, sigma )\n",
    "        \n",
    "    return np.sign(val)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In RBF kernel $k(x_1, x_1) = \\exp(\\frac{\\|x_1 -x_2\\|^2}{-2 \\sigma^2})$. Also written as  $ \\exp(- \\gamma \\|x_1 -x_2\\|^2)$ there is a hyper parameter $\\sigma$ which we need to decide before solving the  problem using optimizer.\n",
    "\n",
    "- For low value of $\\sigma$ or large value of $\\gamma$, area of influence is close to any support vectors. Clearly support vector influence increases as $\\sigma$ increase or $\\gamma$ decreases. It is similary to effect of $\\sigma$ in gussian distribution.\n",
    "\n",
    "\n",
    "\n",
    "Search for $\\sigma.$ \n",
    "\n",
    "<font color = 'red'>Note choose right value of $\\sigma$ is very important in \n",
    "RBF kernel. </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.1836e+02 -2.9770e+02  2e+02  3e-14  2e+00\n",
      " 1: -1.4745e+02 -1.5011e+02  3e+00  3e-14  3e-01\n",
      " 2: -1.5782e+02 -1.5821e+02  4e-01  6e-15  4e-17\n",
      " 3: -1.5782e+02 -1.5782e+02  4e-03  4e-14  6e-17\n",
      " 4: -1.5782e+02 -1.5782e+02  4e-05  2e-14  5e-17\n",
      "Optimal solution found.\n",
      "sigma = 0.05 validation accuracy = 0.5701754385964912\n",
      "updating best model current val 0.5701754385964912 previous val -inf\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.6374e+01 -2.5994e+02  8e+02  2e+01  2e+00\n",
      " 1: -9.2180e+01 -1.9196e+02  1e+02  5e-15  9e-16\n",
      " 2: -1.0747e+02 -1.1639e+02  9e+00  3e-15  6e-16\n",
      " 3: -1.0831e+02 -1.0944e+02  1e+00  2e-15  3e-16\n",
      " 4: -1.0847e+02 -1.0857e+02  1e-01  1e-14  3e-16\n",
      " 5: -1.0849e+02 -1.0850e+02  5e-03  4e-15  3e-16\n",
      " 6: -1.0849e+02 -1.0850e+02  1e-04  9e-15  3e-16\n",
      " 7: -1.0850e+02 -1.0850e+02  1e-06  4e-15  3e-16\n",
      "Optimal solution found.\n",
      "sigma = 1.1178571428571429 validation accuracy = 0.8596491228070176\n",
      "updating best model current val 0.8596491228070176 previous val 0.5701754385964912\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.1575e+01 -1.5570e+02  1e+03  2e+01  2e+00\n",
      " 1: -3.0009e+01 -2.0227e+02  3e+02  5e+00  5e-01\n",
      " 2: -4.2428e+01 -9.4137e+01  6e+01  2e-01  2e-02\n",
      " 3: -5.4026e+01 -6.6398e+01  1e+01  3e-02  4e-03\n",
      " 4: -5.7560e+01 -6.2256e+01  5e+00  2e-14  1e-15\n",
      " 5: -5.9145e+01 -5.9764e+01  6e-01  2e-14  1e-15\n",
      " 6: -5.9452e+01 -5.9487e+01  3e-02  2e-14  1e-15\n",
      " 7: -5.9472e+01 -5.9473e+01  1e-03  3e-15  1e-15\n",
      " 8: -5.9472e+01 -5.9472e+01  4e-05  3e-15  1e-15\n",
      "Optimal solution found.\n",
      "sigma = 2.1857142857142855 validation accuracy = 0.956140350877193\n",
      "updating best model current val 0.956140350877193 previous val 0.8596491228070176\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.7124e+01 -1.2823e+02  1e+03  2e+01  3e+00\n",
      " 1: -5.2103e+01 -1.9484e+02  5e+02  8e+00  8e-01\n",
      " 2: -4.5488e+01 -1.4180e+02  1e+02  9e-01  1e-01\n",
      " 3: -6.7813e+01 -9.8570e+01  3e+01  1e-01  1e-02\n",
      " 4: -7.7839e+01 -9.0389e+01  1e+01  3e-02  4e-03\n",
      " 5: -8.3501e+01 -8.6739e+01  3e+00  6e-15  5e-15\n",
      " 6: -8.5562e+01 -8.5779e+01  2e-01  1e-14  5e-15\n",
      " 7: -8.5718e+01 -8.5723e+01  5e-03  2e-14  5e-15\n",
      " 8: -8.5722e+01 -8.5722e+01  2e-04  7e-15  5e-15\n",
      " 9: -8.5722e+01 -8.5722e+01  8e-06  2e-14  5e-15\n",
      "Optimal solution found.\n",
      "sigma = 3.253571428571428 validation accuracy = 0.9736842105263158\n",
      "updating best model current val 0.9736842105263158 previous val 0.956140350877193\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.0596e+01 -1.3143e+02  1e+03  2e+01  3e+00\n",
      " 1: -7.5473e+01 -2.1224e+02  7e+02  1e+01  1e+00\n",
      " 2: -8.9417e+01 -2.0855e+02  4e+02  6e+00  6e-01\n",
      " 3: -1.0084e+02 -1.8429e+02  1e+02  4e-01  4e-02\n",
      " 4: -1.2879e+02 -1.5451e+02  3e+01  7e-03  7e-04\n",
      " 5: -1.4266e+02 -1.4894e+02  6e+00  9e-14  1e-14\n",
      " 6: -1.4652e+02 -1.4757e+02  1e+00  5e-14  1e-14\n",
      " 7: -1.4736e+02 -1.4738e+02  2e-02  6e-14  1e-14\n",
      " 8: -1.4737e+02 -1.4737e+02  4e-04  3e-14  2e-14\n",
      " 9: -1.4737e+02 -1.4737e+02  5e-06  3e-14  1e-14\n",
      "Optimal solution found.\n",
      "sigma = 4.321428571428571 validation accuracy = 0.9649122807017544\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.5761e+01 -1.4244e+02  1e+03  2e+01  3e+00\n",
      " 1: -8.8145e+01 -2.2765e+02  8e+02  1e+01  1e+00\n",
      " 2: -1.2399e+02 -2.6912e+02  5e+02  7e+00  7e-01\n",
      " 3: -1.6459e+02 -3.2793e+02  3e+02  2e+00  2e-01\n",
      " 4: -2.0096e+02 -2.5923e+02  6e+01  2e-02  2e-03\n",
      " 5: -2.3116e+02 -2.4526e+02  1e+01  3e-03  3e-04\n",
      " 6: -2.3983e+02 -2.4274e+02  3e+00  1e-13  3e-14\n",
      " 7: -2.4219e+02 -2.4227e+02  8e-02  1e-13  3e-14\n",
      " 8: -2.4225e+02 -2.4225e+02  1e-03  5e-14  3e-14\n",
      " 9: -2.4225e+02 -2.4225e+02  2e-05  2e-14  3e-14\n",
      "Optimal solution found.\n",
      "sigma = 5.389285714285714 validation accuracy = 0.9649122807017544\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.1069e+01 -1.5512e+02  1e+03  2e+01  3e+00\n",
      " 1: -9.9009e+01 -2.4598e+02  8e+02  2e+01  2e+00\n",
      " 2: -1.6020e+02 -3.3689e+02  6e+02  8e+00  8e-01\n",
      " 3: -2.6358e+02 -4.8507e+02  4e+02  3e+00  3e-01\n",
      " 4: -3.0557e+02 -4.1632e+02  1e+02  2e-01  2e-02\n",
      " 5: -3.5796e+02 -3.7893e+02  2e+01  3e-03  3e-04\n",
      " 6: -3.7171e+02 -3.7525e+02  4e+00  4e-04  4e-05\n",
      " 7: -3.7452e+02 -3.7466e+02  1e-01  4e-06  4e-07\n",
      " 8: -3.7464e+02 -3.7464e+02  6e-03  1e-07  1e-08\n",
      " 9: -3.7464e+02 -3.7464e+02  1e-04  2e-09  2e-10\n",
      "Optimal solution found.\n",
      "sigma = 6.457142857142856 validation accuracy = 0.9649122807017544\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.6151e+01 -1.6767e+02  1e+03  3e+01  3e+00\n",
      " 1: -1.0902e+02 -2.6515e+02  9e+02  2e+01  2e+00\n",
      " 2: -1.9876e+02 -4.0848e+02  6e+02  9e+00  9e-01\n",
      " 3: -3.6874e+02 -6.3608e+02  6e+02  5e+00  5e-01\n",
      " 4: -4.6253e+02 -6.2864e+02  2e+02  7e-01  7e-02\n",
      " 5: -5.2011e+02 -5.5637e+02  4e+01  3e-02  3e-03\n",
      " 6: -5.4723e+02 -5.4878e+02  2e+00  8e-04  8e-05\n",
      " 7: -5.4851e+02 -5.4854e+02  3e-02  1e-05  1e-06\n",
      " 8: -5.4853e+02 -5.4854e+02  5e-04  1e-07  1e-08\n",
      " 9: -5.4854e+02 -5.4854e+02  5e-06  1e-09  1e-10\n",
      "Optimal solution found.\n",
      "sigma = 7.5249999999999995 validation accuracy = 0.9649122807017544\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.0839e+01 -1.7940e+02  1e+03  3e+01  3e+00\n",
      " 1: -1.1841e+02 -2.8470e+02  9e+02  2e+01  2e+00\n",
      " 2: -2.3825e+02 -4.8162e+02  7e+02  9e+00  9e-01\n",
      " 3: -4.6389e+02 -7.7147e+02  7e+02  6e+00  6e-01\n",
      " 4: -6.5414e+02 -9.0316e+02  4e+02  2e+00  2e-01\n",
      " 5: -7.2458e+02 -7.8460e+02  8e+01  3e-01  3e-02\n",
      " 6: -7.5585e+02 -7.5969e+02  4e+00  3e-03  3e-04\n",
      " 7: -7.5892e+02 -7.5901e+02  9e-02  6e-05  5e-06\n",
      " 8: -7.5898e+02 -7.5899e+02  3e-03  7e-07  7e-08\n",
      " 9: -7.5898e+02 -7.5898e+02  6e-05  7e-09  7e-10\n",
      "Optimal solution found.\n",
      "sigma = 8.592857142857143 validation accuracy = 0.9649122807017544\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.5043e+01 -1.9020e+02  1e+03  3e+01  3e+00\n",
      " 1: -1.2646e+02 -3.0294e+02  1e+03  2e+01  2e+00\n",
      " 2: -2.7566e+02 -5.5156e+02  8e+02  1e+01  9e-01\n",
      " 3: -5.4221e+02 -8.8540e+02  8e+02  8e+00  7e-01\n",
      " 4: -8.9153e+02 -1.2295e+03  6e+02  3e+00  3e-01\n",
      " 5: -9.6933e+02 -1.0738e+03  2e+02  7e-01  7e-02\n",
      " 6: -9.8609e+02 -1.0110e+03  2e+01  2e-13  2e-13\n",
      " 7: -1.0067e+03 -1.0072e+03  5e-01  7e-13  2e-13\n",
      " 8: -1.0071e+03 -1.0071e+03  1e-02  7e-13  2e-13\n",
      " 9: -1.0071e+03 -1.0071e+03  4e-04  1e-13  2e-13\n",
      "Optimal solution found.\n",
      "sigma = 9.660714285714286 validation accuracy = 0.9649122807017544\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.8781e+01 -2.0022e+02  1e+03  3e+01  3e+00\n",
      " 1: -1.3345e+02 -3.2023e+02  1e+03  2e+01  2e+00\n",
      " 2: -3.0598e+02 -6.1146e+02  8e+02  1e+01  1e+00\n",
      " 3: -5.8494e+02 -9.3450e+02  8e+02  9e+00  8e-01\n",
      " 4: -1.0891e+03 -1.4670e+03  7e+02  5e+00  4e-01\n",
      " 5: -1.2645e+03 -1.3884e+03  2e+02  1e+00  9e-02\n",
      " 6: -1.2795e+03 -1.3010e+03  2e+01  3e-02  2e-03\n",
      " 7: -1.2947e+03 -1.2952e+03  6e-01  5e-04  5e-05\n",
      " 8: -1.2950e+03 -1.2950e+03  9e-03  6e-06  5e-07\n",
      " 9: -1.2950e+03 -1.2950e+03  9e-05  6e-08  5e-09\n",
      "Optimal solution found.\n",
      "sigma = 10.72857142857143 validation accuracy = 0.9649122807017544\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.2134e+01 -2.0967e+02  1e+03  3e+01  3e+00\n",
      " 1: -1.4126e+02 -3.4012e+02  1e+03  2e+01  2e+00\n",
      " 2: -3.3781e+02 -6.7438e+02  9e+02  1e+01  1e+00\n",
      " 3: -6.6087e+02 -1.0310e+03  8e+02  9e+00  8e-01\n",
      " 4: -1.3079e+03 -1.7320e+03  8e+02  6e+00  5e-01\n",
      " 5: -1.5808e+03 -1.7905e+03  3e+02  2e+00  1e-01\n",
      " 6: -1.6120e+03 -1.6545e+03  6e+01  2e-01  2e-02\n",
      " 7: -1.6218e+03 -1.6245e+03  3e+00  3e-03  3e-04\n",
      " 8: -1.6237e+03 -1.6238e+03  7e-02  5e-05  5e-06\n",
      " 9: -1.6238e+03 -1.6238e+03  2e-03  6e-07  6e-08\n",
      "10: -1.6238e+03 -1.6238e+03  2e-05  6e-09  6e-10\n",
      "Optimal solution found.\n",
      "sigma = 11.796428571428573 validation accuracy = 0.956140350877193\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.5196e+01 -2.1876e+02  1e+03  3e+01  2e+00\n",
      " 1: -1.4862e+02 -3.5969e+02  1e+03  2e+01  2e+00\n",
      " 2: -3.6215e+02 -7.2607e+02  9e+02  1e+01  1e+00\n",
      " 3: -7.5008e+02 -1.1287e+03  8e+02  9e+00  8e-01\n",
      " 4: -1.4296e+03 -1.8989e+03  9e+02  7e+00  6e-01\n",
      " 5: -1.8236e+03 -2.3344e+03  7e+02  2e+00  2e-01\n",
      " 6: -1.9731e+03 -2.0393e+03  9e+01  3e-01  2e-02\n",
      " 7: -1.9923e+03 -1.9969e+03  5e+00  4e-03  4e-04\n",
      " 8: -1.9956e+03 -1.9957e+03  1e-01  8e-05  7e-06\n",
      " 9: -1.9957e+03 -1.9957e+03  1e-03  9e-07  7e-08\n",
      "10: -1.9957e+03 -1.9957e+03  1e-05  9e-09  7e-10\n",
      "Optimal solution found.\n",
      "sigma = 12.864285714285714 validation accuracy = 0.956140350877193\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.8051e+01 -2.2763e+02  1e+03  3e+01  2e+00\n",
      " 1: -1.5530e+02 -3.7820e+02  1e+03  2e+01  2e+00\n",
      " 2: -3.8074e+02 -7.6791e+02  9e+02  1e+01  1e+00\n",
      " 3: -7.6656e+02 -1.1806e+03  8e+02  1e+01  8e-01\n",
      " 4: -1.9160e+03 -2.4064e+03  9e+02  7e+00  6e-01\n",
      " 5: -2.3648e+03 -2.7202e+03  5e+02  2e+00  2e-01\n",
      " 6: -2.3995e+03 -2.4541e+03  8e+01  2e-01  2e-02\n",
      " 7: -2.4113e+03 -2.4146e+03  4e+00  1e-02  1e-03\n",
      " 8: -2.4122e+03 -2.4122e+03  9e-02  2e-04  2e-05\n",
      " 9: -2.4122e+03 -2.4122e+03  2e-03  3e-06  3e-07\n",
      "10: -2.4122e+03 -2.4122e+03  2e-04  4e-08  3e-09\n",
      "Optimal solution found.\n",
      "sigma = 13.932142857142857 validation accuracy = 0.956140350877193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.0760e+01 -2.3635e+02  1e+03  3e+01  2e+00\n",
      " 1: -1.6144e+02 -3.9580e+02  1e+03  2e+01  2e+00\n",
      " 2: -3.9631e+02 -8.0397e+02  1e+03  1e+01  1e+00\n",
      " 3: -7.5453e+02 -1.2142e+03  9e+02  1e+01  9e-01\n",
      " 4: -1.5040e+03 -2.0626e+03  1e+03  9e+00  7e-01\n",
      " 5: -2.4338e+03 -3.2000e+03  1e+03  6e+00  5e-01\n",
      " 6: -2.8793e+03 -3.0645e+03  3e+02  8e-01  7e-02\n",
      " 7: -2.8718e+03 -2.8849e+03  2e+01  4e-02  4e-03\n",
      " 8: -2.8736e+03 -2.8738e+03  4e-01  7e-04  6e-05\n",
      " 9: -2.8736e+03 -2.8736e+03  1e-02  1e-05  1e-06\n",
      "10: -2.8736e+03 -2.8736e+03  5e-04  2e-07  1e-08\n",
      "11: -2.8736e+03 -2.8736e+03  5e-06  2e-09  1e-10\n",
      "Optimal solution found.\n",
      "sigma = 15.0 validation accuracy = 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "SUPPORT_VECTOR_WEIGHT_THR = 1e-4\n",
    "\n",
    "best_val_accuracy = -np.inf\n",
    "best_support_vectors_info= None\n",
    "for sigma in np.linspace(.05, 15, 15):    \n",
    "    #Let's iterate over sample and fill matrix K\n",
    "    # Maybe we can do following operations in matrix form.\n",
    "    # Going with two loops right now\n",
    "    for i, x1 in enumerate(X_train):\n",
    "        for j, x2 in enumerate(X_train):\n",
    "            #print(x1.shape, x2.shape)\n",
    "            K[i,j] = rbf_kernel(x1, x2, sigma)\n",
    "    \n",
    "    P = cvxopt.matrix(np.outer(y_train, y_train)*K )           \n",
    "    # Let'solve the optimization problem using qudratic solver from cvxopt\n",
    "    sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    alphas = np.array(sol['x'])\n",
    "    svec_loc = alphas > SUPPORT_VECTOR_WEIGHT_THR\n",
    "    svec_alphas = alphas[svec_loc]\n",
    "    \n",
    "    # svecs contains our support vector\n",
    "    svecs, svec_labels = X_train[ np.ravel(svec_loc),:], y_train[ np.ravel(svec_loc)]\n",
    "    # evaluating the intercept for all the support vectors.\n",
    "    svec_wo_pos = []\n",
    "    svec_wo_neg = []\n",
    "    for svec1 ,lbl1 in zip(svecs, svec_labels):\n",
    "        val = 0\n",
    "        for alpha, svec2, lbl2 in  zip(svec_alphas, svecs, svec_labels):\n",
    "            val +=  alpha*lbl2*rbf_kernel(svec1,svec2, sigma )\n",
    "        if lbl1== +1:    \n",
    "            svec_wo_pos.append( val)\n",
    "        elif lbl1 == -1:\n",
    "            svec_wo_neg.append(val)\n",
    "        else:\n",
    "            print('erro support vector label in not +ve or -ve')\n",
    "\n",
    "    #Let use first/ or mean?\n",
    "    #wo = np.mean(svec_wo)\n",
    "    wo = -(max(svec_wo_neg) + min(svec_wo_pos))/2.0\n",
    "    \n",
    "    val_acc = np.mean([ y == predict_label(x, svec_alphas, svecs, svec_labels, wo, sigma)\n",
    "                       for  x, y in  zip(X_val, y_val)])\n",
    "    print('sigma = {} validation accuracy = {}'.format(sigma,val_acc))\n",
    "    if val_acc > best_val_accuracy:\n",
    "        print('updating best model current val {} previous val {}'.format(val_acc, best_val_accuracy))\n",
    "        best_support_vector_info = (sigma, svec_alphas,svecs, svec_labels, wo)\n",
    "        best_val_accuracy = val_acc\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# picking best sigma for accuracy on validation set \n",
    "best_sigma, svec_alphas, svecs, svec_labels, wo = best_support_vector_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing code  to evalue validation and test set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy = 0.9736842105263158\n",
      "Test accuracy = 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "VAccuracy= np.mean([y== predict_label(x,svec_alphas,svecs,svec_labels,wo,best_sigma) for x,y in zip(X_val, y_val)])\n",
    "print('validation accuracy = {}'.format(VAccuracy))\n",
    "TAccuracy= np.mean([y== predict_label(x,svec_alphas,svecs,svec_labels,wo,best_sigma) for x,y in zip(X_test, y_test)])\n",
    "print('Test accuracy = {}'.format(TAccuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn kernel SVM with soft margin interface. Select best model and evaluate accuracy on test set using SVM from  the library\n",
    "\n",
    "Check your accuracy value matches\n",
    "\n",
    "Hint\n",
    "\n",
    "- You need to use large value of C to simulate hard margin SVM\n",
    "- gamma= 1/(2*best_sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.07683863885839738,\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "gamma= 1/(2*best_sigma*2)\n",
    "clf = svm.SVC(C= 1, gamma= gamma)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25197354])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12669429179001424"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
